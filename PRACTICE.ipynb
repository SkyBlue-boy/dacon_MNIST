{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 ; https://datascienceschool.net/view-notebook/2205ad8f0c5947c08696e8927b466341/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.567894230\n",
      "Epoch: 0002, Cost: 1.057073300\n",
      "Epoch: 0003, Cost: 0.852373450\n",
      "Epoch: 0004, Cost: 0.751388045\n",
      "Epoch: 0005, Cost: 0.686439359\n",
      "Epoch: 0006, Cost: 0.640169789\n",
      "Epoch: 0007, Cost: 0.605904847\n",
      "Epoch: 0008, Cost: 0.577114308\n",
      "Epoch: 0009, Cost: 0.553933760\n",
      "Epoch: 0010, Cost: 0.534603632\n",
      "Epoch: 0011, Cost: 0.517797853\n",
      "Epoch: 0012, Cost: 0.503041457\n",
      "Epoch: 0013, Cost: 0.489816927\n",
      "Epoch: 0014, Cost: 0.478698725\n",
      "Epoch: 0015, Cost: 0.467733410\n",
      "Learning finished\n",
      "Accuracy:  0.8898\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "# 1단 네트워크 형성\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN for MNIST(3단 네트워크로 설정함) + relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 176.281539551\n",
      "Epoch: 0002 cost = 41.908027923\n",
      "Epoch: 0003 cost = 26.298391132\n",
      "Epoch: 0004 cost = 18.145404683\n",
      "Epoch: 0005 cost = 13.052144751\n",
      "Epoch: 0006 cost = 9.716397311\n",
      "Epoch: 0007 cost = 7.202048320\n",
      "Epoch: 0008 cost = 5.574565104\n",
      "Epoch: 0009 cost = 4.118838510\n",
      "Epoch: 0010 cost = 2.969157610\n",
      "Epoch: 0011 cost = 2.248789161\n",
      "Epoch: 0012 cost = 1.694369281\n",
      "Epoch: 0013 cost = 1.317366233\n",
      "Epoch: 0014 cost = 1.028091546\n",
      "Epoch: 0015 cost = 0.768396142\n",
      "Learning Finished!\n",
      "Accuracy: 0.9422\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders \n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers # 3단 네트워크 형성 # 중간 256개로 디자인 # 처음은 784개, 마지막은 10개로 정해짐(input, output)\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2번에서 + 초기화 잘하기(초기값 바꾸기) ; Xavier 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.310728118\n",
      "Epoch: 0002 cost = 0.117652805\n",
      "Epoch: 0003 cost = 0.077500304\n",
      "Epoch: 0004 cost = 0.056122337\n",
      "Epoch: 0005 cost = 0.040922519\n",
      "Epoch: 0006 cost = 0.031875485\n",
      "Epoch: 0007 cost = 0.026655684\n",
      "Epoch: 0008 cost = 0.020816645\n",
      "Epoch: 0009 cost = 0.015489178\n",
      "Epoch: 0010 cost = 0.015900443\n",
      "Epoch: 0011 cost = 0.014517088\n",
      "Epoch: 0012 cost = 0.009339009\n",
      "Epoch: 0013 cost = 0.013273051\n",
      "Epoch: 0014 cost = 0.009179672\n",
      "Epoch: 0015 cost = 0.008998190\n",
      "Learning Finished!\n",
      "Accuracy: 0.9785\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 10 MNIST and Xavier\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NN for MNIST \n",
    "3번결과(0.9785) 보다 지금 실행시킨 코드에서 결과(0.9761)값이 더 낮은 이유는 아마도 네트워크가 깊어지면 학습데이터를 너무 잘 기억해서 새로운 테스트 데이터가 들어왔을때 오히려 결과가 나빠지는 현상(overfitting)때문 일것이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.296638294\n",
      "Epoch: 0002 cost = 0.105326774\n",
      "Epoch: 0003 cost = 0.071272261\n",
      "Epoch: 0004 cost = 0.052205828\n",
      "Epoch: 0005 cost = 0.038402571\n",
      "Epoch: 0006 cost = 0.036553998\n",
      "Epoch: 0007 cost = 0.033010044\n",
      "Epoch: 0008 cost = 0.026166803\n",
      "Epoch: 0009 cost = 0.021202014\n",
      "Epoch: 0010 cost = 0.021440563\n",
      "Epoch: 0011 cost = 0.020498195\n",
      "Epoch: 0012 cost = 0.015944619\n",
      "Epoch: 0013 cost = 0.015583333\n",
      "Epoch: 0014 cost = 0.018211059\n",
      "Epoch: 0015 cost = 0.013665459\n",
      "Learning Finished!\n",
      "Accuracy: 0.9761\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()를 코드 첫부분에 추가해주시면 \n",
    "# 매 실행때마다 tensorflow에서 이전에 정의한 데이터들을 초기화 하게 되고 위와 같은 문제가 발생하지 않습니다.\n",
    "# tensorflow에서 변수 해제하기\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Lab 10 MNIST and Deep learning\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "# 256 -> 512 로 바꾸기\n",
    "# 5단 네트워크 형성\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout for MNIST\n",
    "dropout ; 네트워크가 overfitting 되지 않도록 일부 네트워크를 끊고 학습시킨다\n",
    "이를 tensorflow에서 구현하려면 한 레이어를 더 추가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 처음시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-10-d8ba87090e48>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch: 0001 cost = 0.469013569\n",
      "Epoch: 0002 cost = 0.169212850\n",
      "Epoch: 0003 cost = 0.129919363\n",
      "Epoch: 0004 cost = 0.106076214\n",
      "Epoch: 0005 cost = 0.089780703\n",
      "Epoch: 0006 cost = 0.082527091\n",
      "Epoch: 0007 cost = 0.074385894\n",
      "Epoch: 0008 cost = 0.067673927\n",
      "Epoch: 0009 cost = 0.062669368\n",
      "Epoch: 0010 cost = 0.062335250\n",
      "Epoch: 0011 cost = 0.057973712\n",
      "Epoch: 0012 cost = 0.053018946\n",
      "Epoch: 0013 cost = 0.050590273\n",
      "Epoch: 0014 cost = 0.046729017\n",
      "Epoch: 0015 cost = 0.043457833\n",
      "Learning Finished!\n",
      "Accuracy: 0.9821\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()를 코드 첫부분에 추가해주시면 \n",
    "# 매 실행때마다 tensorflow에서 이전에 정의한 데이터들을 초기화 하게 되고 위와 같은 문제가 발생하지 않습니다.\n",
    "# tensorflow에서 변수 해제하기\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Lab 10 MNIST and Dropout\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "# 통상적으로 train 할 때는 0.5~0.7 정도, test 에서는 무조건 1\n",
    "# keep_prob : 전체중에 몇 프로의 네트워크를 keep할 것인가\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두번째 시도(오류 수정: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.) 뭔가 이상하다(Accuracy: 0.098)--> 다시 고치자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 1.921656105\n",
      "Epoch: 0002 cost = 0.671262528\n",
      "Epoch: 0003 cost = 0.484860237\n",
      "Epoch: 0004 cost = 0.398666654\n",
      "Epoch: 0005 cost = 0.361347553\n",
      "Epoch: 0006 cost = 0.330897928\n",
      "Epoch: 0007 cost = 0.316777199\n",
      "Epoch: 0008 cost = 0.300665119\n",
      "Epoch: 0009 cost = 0.283485364\n",
      "Epoch: 0010 cost = 0.273668158\n",
      "Epoch: 0011 cost = 0.265803824\n",
      "Epoch: 0012 cost = 0.259918343\n",
      "Epoch: 0013 cost = 0.257040729\n",
      "Epoch: 0014 cost = 0.251677054\n",
      "Epoch: 0015 cost = 0.250219988\n",
      "Learning Finished!\n",
      "Accuracy: 0.098\n",
      "Label:  [8]\n",
      "Prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()를 코드 첫부분에 추가해주시면 \n",
    "# 매 실행때마다 tensorflow에서 이전에 정의한 데이터들을 초기화 하게 되고 위와 같은 문제가 발생하지 않습니다.\n",
    "# tensorflow에서 변수 해제하기\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Lab 10 MNIST and Dropout\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "# 통상적으로 train 할 때는 0.5~0.7 정도, test 에서는 무조건 1\n",
    "# keep_prob : 전체중에 몇 프로의 네트워크를 keep할 것인가\n",
    "rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, rate=rate)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, rate=rate)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, rate=rate)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, rate=rate)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, rate: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, rate: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], rate: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other optimizers 사용해보기\n",
    "AdadeltaOptimizer, AdagradOptimizer, AdagradDAOptimizer, MomentumOptimizer\n",
    "AdamOptimizer, FtrlOptimizer, ProximalGradientDescentOptimizer, \n",
    "ProximalAdagradOptimizer, RMSPropOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization 하기 ; 입력값을 normalize 잘하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xavier 랑 dropout을 deep 3에 넣으니까 정확도가 높아졌어요! \n",
    "deep5 일때는 98.04 인데 deep 3하니까 98.34나오네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
